{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Digit_Recognizer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwEIOKZKEQma",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning Exercise 2\n",
        "Χατζηγιανέλλης Ιγνάτιος\n",
        "Α.Μ.: MTN1915"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRaGd6B0fqPR",
        "colab_type": "text"
      },
      "source": [
        "### Προετοιμασία περιβάλλοντος\n",
        "Φορτώνουμε βιβλιοθήκες και το dataset μας"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbWiqfEdEQmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC, LinearSVC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gp7sklfjEQmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/MSc-in-Data-Science/class_material/master/semester_1/Machine_Learning/datasets/digit_recognizer_dataset.csv'\n",
        "data = pd.read_csv(url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJAFfloUEQml",
        "colab_type": "text"
      },
      "source": [
        "### Ανάλυση και προετοιμασία δεδομένων\n",
        "Κοιτώντας τα στατιστικά από τα δεδομένα, φαίνεται πως οι κλάσεις εμφανίζονται με μια ομοιόμορφη κατανομή,\n",
        "καθώς επίσης δεν υπάρχουν κενά features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW6n_Gb8EQmm",
        "colab_type": "code",
        "outputId": "3d981444-7d1c-41c8-c3cd-99114a9f680f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>pixel0</th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>pixel9</th>\n",
              "      <th>pixel10</th>\n",
              "      <th>pixel11</th>\n",
              "      <th>pixel12</th>\n",
              "      <th>pixel13</th>\n",
              "      <th>pixel14</th>\n",
              "      <th>pixel15</th>\n",
              "      <th>pixel16</th>\n",
              "      <th>pixel17</th>\n",
              "      <th>pixel18</th>\n",
              "      <th>pixel19</th>\n",
              "      <th>pixel20</th>\n",
              "      <th>pixel21</th>\n",
              "      <th>pixel22</th>\n",
              "      <th>pixel23</th>\n",
              "      <th>pixel24</th>\n",
              "      <th>pixel25</th>\n",
              "      <th>pixel26</th>\n",
              "      <th>pixel27</th>\n",
              "      <th>pixel28</th>\n",
              "      <th>pixel29</th>\n",
              "      <th>pixel30</th>\n",
              "      <th>pixel31</th>\n",
              "      <th>pixel32</th>\n",
              "      <th>pixel33</th>\n",
              "      <th>pixel34</th>\n",
              "      <th>pixel35</th>\n",
              "      <th>pixel36</th>\n",
              "      <th>pixel37</th>\n",
              "      <th>pixel38</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel744</th>\n",
              "      <th>pixel745</th>\n",
              "      <th>pixel746</th>\n",
              "      <th>pixel747</th>\n",
              "      <th>pixel748</th>\n",
              "      <th>pixel749</th>\n",
              "      <th>pixel750</th>\n",
              "      <th>pixel751</th>\n",
              "      <th>pixel752</th>\n",
              "      <th>pixel753</th>\n",
              "      <th>pixel754</th>\n",
              "      <th>pixel755</th>\n",
              "      <th>pixel756</th>\n",
              "      <th>pixel757</th>\n",
              "      <th>pixel758</th>\n",
              "      <th>pixel759</th>\n",
              "      <th>pixel760</th>\n",
              "      <th>pixel761</th>\n",
              "      <th>pixel762</th>\n",
              "      <th>pixel763</th>\n",
              "      <th>pixel764</th>\n",
              "      <th>pixel765</th>\n",
              "      <th>pixel766</th>\n",
              "      <th>pixel767</th>\n",
              "      <th>pixel768</th>\n",
              "      <th>pixel769</th>\n",
              "      <th>pixel770</th>\n",
              "      <th>pixel771</th>\n",
              "      <th>pixel772</th>\n",
              "      <th>pixel773</th>\n",
              "      <th>pixel774</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.00000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.00000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.00000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>4.456643</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00300</td>\n",
              "      <td>0.011190</td>\n",
              "      <td>0.005143</td>\n",
              "      <td>0.000214</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000381</td>\n",
              "      <td>0.001310</td>\n",
              "      <td>0.010548</td>\n",
              "      <td>0.027262</td>\n",
              "      <td>0.050905</td>\n",
              "      <td>0.066405</td>\n",
              "      <td>0.129571</td>\n",
              "      <td>...</td>\n",
              "      <td>3.772524</td>\n",
              "      <td>2.748905</td>\n",
              "      <td>1.796452</td>\n",
              "      <td>1.089905</td>\n",
              "      <td>0.563190</td>\n",
              "      <td>0.239571</td>\n",
              "      <td>0.093524</td>\n",
              "      <td>0.024833</td>\n",
              "      <td>0.000857</td>\n",
              "      <td>0.001405</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.006143</td>\n",
              "      <td>0.035833</td>\n",
              "      <td>0.082357</td>\n",
              "      <td>0.114905</td>\n",
              "      <td>0.178714</td>\n",
              "      <td>0.301452</td>\n",
              "      <td>0.413643</td>\n",
              "      <td>0.513667</td>\n",
              "      <td>0.558833</td>\n",
              "      <td>0.677857</td>\n",
              "      <td>0.60281</td>\n",
              "      <td>0.489238</td>\n",
              "      <td>0.340214</td>\n",
              "      <td>0.219286</td>\n",
              "      <td>0.117095</td>\n",
              "      <td>0.059024</td>\n",
              "      <td>0.02019</td>\n",
              "      <td>0.017238</td>\n",
              "      <td>0.002857</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.887730</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.56812</td>\n",
              "      <td>1.626927</td>\n",
              "      <td>1.053972</td>\n",
              "      <td>0.043916</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.078072</td>\n",
              "      <td>0.232634</td>\n",
              "      <td>1.131661</td>\n",
              "      <td>2.310396</td>\n",
              "      <td>3.121847</td>\n",
              "      <td>3.259128</td>\n",
              "      <td>4.992894</td>\n",
              "      <td>...</td>\n",
              "      <td>26.957829</td>\n",
              "      <td>22.879248</td>\n",
              "      <td>18.595109</td>\n",
              "      <td>14.434439</td>\n",
              "      <td>10.517823</td>\n",
              "      <td>6.469315</td>\n",
              "      <td>3.976306</td>\n",
              "      <td>1.846016</td>\n",
              "      <td>0.139556</td>\n",
              "      <td>0.287891</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.949803</td>\n",
              "      <td>2.350859</td>\n",
              "      <td>3.934280</td>\n",
              "      <td>4.543583</td>\n",
              "      <td>5.856772</td>\n",
              "      <td>7.219742</td>\n",
              "      <td>8.928286</td>\n",
              "      <td>10.004069</td>\n",
              "      <td>10.129595</td>\n",
              "      <td>11.254931</td>\n",
              "      <td>10.69603</td>\n",
              "      <td>9.480066</td>\n",
              "      <td>7.950251</td>\n",
              "      <td>6.312890</td>\n",
              "      <td>4.633819</td>\n",
              "      <td>3.274488</td>\n",
              "      <td>1.75987</td>\n",
              "      <td>1.894498</td>\n",
              "      <td>0.414264</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>7.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>9.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>116.00000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>216.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>47.000000</td>\n",
              "      <td>157.000000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>243.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>253.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>59.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>177.000000</td>\n",
              "      <td>231.000000</td>\n",
              "      <td>253.000000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.00000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>253.000000</td>\n",
              "      <td>253.00000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>62.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              label   pixel0   pixel1  ...  pixel781  pixel782  pixel783\n",
              "count  42000.000000  42000.0  42000.0  ...   42000.0   42000.0   42000.0\n",
              "mean       4.456643      0.0      0.0  ...       0.0       0.0       0.0\n",
              "std        2.887730      0.0      0.0  ...       0.0       0.0       0.0\n",
              "min        0.000000      0.0      0.0  ...       0.0       0.0       0.0\n",
              "25%        2.000000      0.0      0.0  ...       0.0       0.0       0.0\n",
              "50%        4.000000      0.0      0.0  ...       0.0       0.0       0.0\n",
              "75%        7.000000      0.0      0.0  ...       0.0       0.0       0.0\n",
              "max        9.000000      0.0      0.0  ...       0.0       0.0       0.0\n",
              "\n",
              "[8 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_n4nh9VbEQmq",
        "colab_type": "code",
        "outputId": "ff13cb85-4d7c-4a02-a5b7-d8fe41f608c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "data.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label       0\n",
              "pixel0      0\n",
              "pixel1      0\n",
              "pixel2      0\n",
              "pixel3      0\n",
              "           ..\n",
              "pixel779    0\n",
              "pixel780    0\n",
              "pixel781    0\n",
              "pixel782    0\n",
              "pixel783    0\n",
              "Length: 785, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRn80fb7EQmu",
        "colab_type": "text"
      },
      "source": [
        "Διαχωρίζουμε από τα data το label στη μεταβλητή y και κρατάμε τα υπόλοιπα στη μεταβλητή X"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5Ir2yN1FbIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data.drop('label', axis=1)\n",
        "y = data.label\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvYkP8EQUZYV",
        "colab_type": "text"
      },
      "source": [
        "Κάνοντας ένα plot των labels, φαίνεται πως η κατανομή τους είναι αρκετά ομοιόμορφη με την κλάση για τον αριθμό 1 να υπερέχει ελάχιστα. Παρ'όλα αυτά αντιπροσωπεύονται σε πολύ καλό βαθμό όλες οι κλάσεις."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECyTha_YUJiR",
        "colab_type": "code",
        "outputId": "a7dd3517-1df0-4732-d69d-8d5e6ad0dbb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "sns.countplot(y)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd66WpbyFoXj",
        "colab_type": "text"
      },
      "source": [
        "Κάνοντας μια ανάλυση PCA παρατηρούμε ότι ακόμα και μειώνοντας τις διαστάσεις των features κοντά στις 200 από 784, μπορούμε να κρατήσουμε κοντά στο 95% τη διακύμανση από τα αρχικά δεδομένα"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ivax6nRjakBw",
        "colab_type": "code",
        "outputId": "6ef038c0-9ce5-4f79-fd2c-4c497be73c9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "pca = PCA().fit(X)\n",
        "\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('Cumulative explained variance');"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEiBUyIQESlwSgQiIgVOiUBEpMD9f/RO7/3o0+6AAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dltXCel4F-f0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=200)\n",
        "pca.fit(X_train)\n",
        "X_train_pca = pca.transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "095AVIWSEQmy",
        "colab_type": "text"
      },
      "source": [
        "### Επιλογή αλγορίθμου και μετρικών\n",
        "Από τη στιγμή που έχουμε ένα classification πρόβλημα, οι γνωστές μετρικές που μπορούμε να αξιοποιήσουμε για το evaluation των μοντέλων μας είναι το precision, recall και κυριώς ο συνδυασμός τους, το F1 Score.\n",
        "\n",
        "Ξεκινώντας με την αναζήτηση του κατάλληλου αλγορίθμιου για την εκπαίδευση του μοντέλου, δοκιμάζουμε την αποτελεσματικότητα 5 διαφορετικών αλγορίθμων, των Logistic Regression, KNN, Decision Trees, Naive Bayes και SVM. Κρατάμε τα μοντέλα σε απλή μορφή ώστε να δούμε την αρχική τους απόδοση, να κάνουμε μια αρχική εκτίμηση του προβλήματος και να επιλέξουμε με ποιά θα συνεχίσουμε."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcST0Szd4scX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = []\n",
        "models.append(('LR',  LogisticRegression()))\n",
        "models.append(('kNN', KNeighborsClassifier()))\n",
        "models.append(('DT',  DecisionTreeClassifier()))\n",
        "models.append(('NB',  GaussianNB()))\n",
        "models.append(('SVM', SVC()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNhvVvZE44l7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scoring = 'f1_micro'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFjaUZbmOI-H",
        "colab_type": "text"
      },
      "source": [
        "Αφού μαζέψαμε όλους τους υποψήφιους αλγορίθμους, τους τρέξαμε με τα ίδια κριτήρια και επομένως εφαρμόσαμε 5-Fold Cross Validation και παρατηρήθηκαν τα εξής αποτελέσματα\n",
        "\n",
        "\n",
        "*   Ο Naive Bayes λόγω της αρχής ανεξαρτησίας των χαρακτηριστικών δεν κατάφερε να βρει καλό αποτέλεσμα\n",
        "*   Οι KNN και Logistic Regression τα πήγαν αρκετά καλά αλλά όχι τόσο όσο άλλοι σε επίπεδο χρόνου και αποτελεσματικότητας\n",
        "*   Ο SVM τα πήγε πολύ καλά και με μικρή διακύμανση στα πιθανά accuracies, επομένως σίγουρα θα είναι υποψήφιος\n",
        "*   Τα Decision Trees, αν και πολύ απλά καθώς και επιρρεπή στο overfitting με τόσα χαρακτηριστικά, τα πήγαν αρκετά καλά, οπότε στη συνέχεια θα μελετήσουμε μια πιο εύρωστη εκδοχή τους, τα Random Forests\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IflicN8b4509",
        "colab_type": "code",
        "outputId": "0a6a484f-813a-4ada-f656-2fd134f5780c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "source": [
        "results = []\n",
        "names   = []\n",
        "for name, model in models:\n",
        "  cv_results = cross_val_score(model, X, y, cv=5, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  print(\"%03s: %f (+/- %f)\" % (name, cv_results.mean(), cv_results.std()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " LR: 0.915881 (+/- 0.001925)\n",
            "kNN: 0.965857 (+/- 0.002287)\n",
            " DT: 0.854452 (+/- 0.003033)\n",
            " NB: 0.557262 (+/- 0.003288)\n",
            "SVM: 0.975524 (+/- 0.001371)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQnbRJiETyeI",
        "colab_type": "text"
      },
      "source": [
        "Ξεκινώντας με τα Random Forest, μιας και η παραμετροποίησή τους είναι σχετικά μεγάλη, φτιάχνουμε ενα grid με πιθανές τιμές για βασικές παραμέτρους και εφαρμόζουμε Randomized Search Cross Validation ώστε να έχουμε ένα περιορίσουμε λίγο μερικές από αυτές σε μια καλή αρχή.\n",
        "\n",
        "Δυστυχώς ο χρόνος που απαιτείται για την αναζήτηση είναι πολύς και συνεπώς δεν προλαβαίνουμε να κάνουμε Grid Search. Οι πιθανοί συνδυασμοί που θα ψάξει είναι 20 και κάθε φορά θα κάνει 3-Fold Cros Validation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bctGzRUDfTE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_estimators = [int(x) for x in np.linspace(100, 1000, num = 50)]\n",
        "max_features = ['auto', 'sqrt']\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "min_samples_split = [2, 5, 10]\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "bootstrap = [True, False]\n",
        "\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib8tBhmrEP7a",
        "colab_type": "code",
        "outputId": "7d1757f2-efd2-46b0-d5d5-50926cf2916c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "rf = RandomForestClassifier()\n",
        "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 20, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
        "rf_random.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed: 43.6min\n",
            "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 63.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3, error_score=nan,\n",
              "                   estimator=RandomForestClassifier(bootstrap=True,\n",
              "                                                    ccp_alpha=0.0,\n",
              "                                                    class_weight=None,\n",
              "                                                    criterion='gini',\n",
              "                                                    max_depth=None,\n",
              "                                                    max_features='auto',\n",
              "                                                    max_leaf_nodes=None,\n",
              "                                                    max_samples=None,\n",
              "                                                    min_impurity_decrease=0.0,\n",
              "                                                    min_impurity_split=None,\n",
              "                                                    min_samples_leaf=1,\n",
              "                                                    min_samples_split=2,\n",
              "                                                    min_weight_fraction_leaf=0.0,\n",
              "                                                    n_estimators=100,\n",
              "                                                    n_jobs...\n",
              "                                                      70, 80, 90, 100, 110,\n",
              "                                                      None],\n",
              "                                        'max_features': ['auto', 'sqrt'],\n",
              "                                        'min_samples_leaf': [1, 2, 4],\n",
              "                                        'min_samples_split': [2, 5, 10],\n",
              "                                        'n_estimators': [100, 118, 136, 155,\n",
              "                                                         173, 191, 210, 228,\n",
              "                                                         246, 265, 283, 302,\n",
              "                                                         320, 338, 357, 375,\n",
              "                                                         393, 412, 430, 448,\n",
              "                                                         467, 485, 504, 522,\n",
              "                                                         540, 559, 577, 595,\n",
              "                                                         614, 632, ...]},\n",
              "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
              "                   return_train_score=False, scoring=None, verbose=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVyiC0f-ibbf",
        "colab_type": "code",
        "outputId": "01fcdd10-647c-42fa-ebc1-f23a0aef2a02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "rf_random.best_params_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bootstrap': False,\n",
              " 'max_depth': 60,\n",
              " 'max_features': 'sqrt',\n",
              " 'min_samples_leaf': 1,\n",
              " 'min_samples_split': 2,\n",
              " 'n_estimators': 926}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co1Wod7ptRWG",
        "colab_type": "text"
      },
      "source": [
        "Με τα αποτελέσματα του Randomized Search και μετά από λίγες μικρές αλλαγές και δοκιμές, τα Random Forest φαίνεται να τα πηγαίνουν πολύ καλά"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHG4oJ9PjXEb",
        "colab_type": "code",
        "outputId": "3a3d1bd8-5b46-4686-ed4c-238401f4d0d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "rf_clf = RandomForestClassifier(n_estimators=925, max_depth=55, max_features='sqrt', min_samples_leaf=1, min_samples_split=2, bootstrap=False)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "\n",
        "print('Score on train', round(f1_score(y_train, rf_clf.predict(X_train), average='micro'), 3))\n",
        "print('Score on test', round(f1_score(y_test, y_pred, average='micro'), 3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score on train 1.0\n",
            "Score on test 0.969\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkVCb-0jtkOF",
        "colab_type": "text"
      },
      "source": [
        "Δοκιμάζοντας και με τα PCA features και φυσικά μειώνοντας τα δέντρα και το βάθος αφού έχουμε πολύ λιγότερα χαρακτηριστικά, η απόδοση είναι κοντά στην αρχική με μικρότερους χρόνους εκπαίδευσης"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHvbVfZpmk5T",
        "colab_type": "code",
        "outputId": "7025024a-f581-4a24-baf5-798ee7d4d604",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "rf_clf = RandomForestClassifier(n_estimators=600, max_depth=40, max_features='sqrt', min_samples_leaf=1, min_samples_split=2, bootstrap=True)\n",
        "rf_clf.fit(X_train_pca, y_train)\n",
        "y_pred = rf_clf.predict(X_test_pca)\n",
        "\n",
        "print('Score on train', round(f1_score(y_train, rf_clf.predict(X_train_pca), average='micro'), 3))\n",
        "print('Score on test', round(f1_score(y_test, y_pred, average='micro'), 3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score on train 1.0\n",
            "Score on test 0.944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRYNpIBMjnvh",
        "colab_type": "text"
      },
      "source": [
        "Προχωρώντας στον SVM, ο χρόνος και τα resources (out of stack) που χρειαζόταν για να κάνει Randomized Search και Grid Search δε μου επέτρεψαν να εκτελέσω τους αλγορίθμους και επομένως πειραματίστηκα με τιμές.\n",
        "\n",
        "Το αποτέλεσμα παρ'όλα αυτά ήταν εντυπωσιακό."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeRIUxQtNTol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svm = SVC(C=10, gamma=0.001, kernel=\"rbf\")\n",
        "\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "print('Score on train', round(f1_score(y_train, svm.predict(X_train), average='micro'), 3))\n",
        "print('Score on test', round(f1_score(y_test, y_pred, average='micro'), 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLQ6XhTswLyV",
        "colab_type": "text"
      },
      "source": [
        "Δοκιμάζοντας τον SVM στα δεδομένα που είχαμε εφαρμόσει PCA τα αποτελέσματα είναι εξίσου καλά."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cSVTfVqs8Wu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6d9575ad-2578-4204-a6d5-ebc4ec7ee7dd"
      },
      "source": [
        "svm = SVC(kernel=\"rbf\")\n",
        "svm.fit(X_train_pca, y_train)\n",
        "y_pred = svm.predict(X_test_pca)\n",
        "\n",
        "print('Score on train', round(f1_score(y_train, svm.predict(X_train_pca), average='micro'), 3))\n",
        "print('Score on test', round(f1_score(y_test, y_pred, average='micro'), 3))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score on train 0.993\n",
            "Score on test 0.978\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3MnBmDnu2Kf",
        "colab_type": "text"
      },
      "source": [
        "Εφαρμόζουμε δοκιμαστικά και γραμμικό SVM και συγκεκριμένα το LinearSVC και όχι αλλάζοντας την παράμετρο kernel του SVC μοντέλου μιας και το sklearn το προτείνει ως περισσότερο optimized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8M5DwVJkPer",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "34c9cec2-f858-4bdd-ee0c-818de497da87"
      },
      "source": [
        "svm_linear = LinearSVC(C=10, multi_class='ovr', max_iter=50000)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred = svm_linear.predict(X_test)\n",
        "\n",
        "print('Score on train', round(f1_score(y_train, svm_linear.predict(X_train), average='micro'), 3))\n",
        "print('Score on test', round(f1_score(y_test, y_pred, average='micro'), 3))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Score on train 0.877\n",
            "Score on test 0.846\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7AyEAFCvI4E",
        "colab_type": "text"
      },
      "source": [
        "### Train του τελικού μοντέλου μας"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olBmRQ8dvNMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svm = SVC(C=10, gamma=0.001, kernel=\"rbf\")\n",
        "svm.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}